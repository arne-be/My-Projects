{"cells":[{"cell_type":"markdown","metadata":{"id":"IvcOz1eTrxRx"},"source":["# **Deep Generative Models: Variational Autoencoders and Generative Adversarial Networks**"]},{"cell_type":"markdown","metadata":{"id":"EvWs2XIYNfhg"},"source":["# **CK Dataset**\n","In the following exercices, you will work with images extracted from the CK dataset: http://www.jeffcohn.net/wp-content/uploads/2020/02/Cohn-Kanade_Database.pdf.pdf\n","\n","It contains gray-scale images of human faces.\n","\n","The dataset is provided in the folder Data/faces/ in .mat format.\n","In the following we provide a Dataset class in pytorch to load images from this database."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"elapsed":3804,"status":"ok","timestamp":1655742860674,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"5kE5D0hh1Ndz","outputId":"4b30798b-2121-4c72-93d8-6dbcf7695d17"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","torch.Size([256, 1, 64, 64])\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 432x288 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["## Create a Custom Dataset for CK database\n","import scipy.io as sio\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/DeepLearning_2022/P4/Data/'\n","results_path = '/content/drive/My Drive/DeepLearning_2022/P4/Results/'\n","\n","# All the data will be loaded from the provided file in Data/mnist.t\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import matplotlib.pyplot as plt\n","from IPython.display import Image as ImageDisp\n","import scipy.io as sio\n","\n","#vis_task imports\n","from torchvision.utils import make_grid\n","import imageio\n","\n","\n","#Making native class loader\n","class FacesDB(torch.utils.data.Dataset):\n","    # Initialization method for the dataset\n","    def __init__(self,dataDir = data_path+'/faces/face_ims_64x64.mat', transform = None):\n","        mat_loaded = sio.loadmat(dataDir)\n","        self.data = mat_loaded['X']\n","        self.transform = transform\n","\n","    # What to do to load a single item in the dataset ( read image and label)    \n","    def __getitem__(self, index):\n","        data = self.data[:,:,0,index]   \n","        data = Image.fromarray(data,mode='L')\n","        # Apply a trasnformaiton to the image if it is indicated in the initalizer\n","        if self.transform is not None : \n","            data = self.transform(data)\n","        \n","        # return the image and the label\n","        return data\n","\n","    # Return the number of images\n","    def __len__(self):\n","        return self.data.shape[3]\n","\n","import torchvision.transforms as transforms\n","\n","tr = transforms.Compose([\n","        tf.Resize((64,64)),\n","        transforms.ToTensor(), \n","        ])\n","faces_db = FacesDB(data_path+'/faces/face_ims_64x64.mat',tr)\n","train_loader = torch.utils.data.DataLoader(dataset=faces_db,\n","                                           batch_size=256, \n","                                           shuffle=True)\n","\n","#sample for vis_task\n","ck_sample = torch.utils.data.DataLoader(dataset=faces_db,\n","                                           batch_size=16, \n","                                           shuffle=True)\n","\n","\n","# Mini-batch images\n","images = next(iter(train_loader))\n","print(images.shape)\n","image = images[0,:,:,:].repeat(3,1,1)\n","plt.imshow(image.permute(1,2,0).squeeze().numpy())\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CVA6m-IgNace"},"source":["# Ex. 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1653800688561,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"y1yTECVjDVmf","outputId":"5285d823-5808-45c8-bbd9-c18510b02767"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\\n2. For every two epochs during training:\\n  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\\n  2.2. Generate and show a set of images from random noise z. \\n  2.3. Visualize a set of generated images by interpolating over the latent space z.\\n  2.4 Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\\n'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4 Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"markdown","metadata":{"id":"TPhaB5uN5nub"},"source":["## Sol. 1"]},{"cell_type":"markdown","metadata":{"id":"5vCN2T2o5Ww5"},"source":["**Defining VAE**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rk4wOvmy2wiw"},"outputs":[],"source":["# Convolution + BatchNormnalization + ReLU block for the encoder\n","class ConvBNReLU(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(ConvBNReLU, self).__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.AvgPool2d(2,2)\n","\n","  def forward(self,x):\n","    if(self.pool):\n","      out = self.pool(x)\n","    else:\n","      out = x\n","    out = self.relu(self.bn(self.conv(out)))   \n","    return out\n","\n","#  BatchNormnalization + ReLU block + Convolution for the decoder\n","class BNReLUConv(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(BNReLUConv, self).__init__()\n","    self.bn = nn.BatchNorm2d(in_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.UpsamplingNearest2d(scale_factor=2)\n","\n","  def forward(self,x):\n","    out = self.relu(self.bn(x))\n","    if(self.pool):\n","      out = self.pool(out)\n","    out = self.conv(out)\n","    return out\n","\n","# Encoder definition with 3 COnv-BN-ReLU blocks and fully-connected layer\n","class Encoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Encoder, self).__init__()\n","    self.layer1 = ConvBNReLU(1,base_channels,pooling=False)\n","    self.layer2 = ConvBNReLU(base_channels,base_channels*2,pooling=True)\n","    self.layer3 = ConvBNReLU(base_channels*2,base_channels*4,pooling=True)\n","    self.fc = nn.Linear(16*16*base_channels*4,out_features)       #8 changed to 16 as we have 64x64 images instead of 32x32\n","  \n","  def forward(self,x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    return self.fc(out.view(x.shape[0],-1))\n","    \n","# Decoder definition with a fully-connected layer and 3 BN-ReLU-COnv blocks and \n","class Decoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Decoder, self).__init__()\n","    self.base_channels = base_channels\n","    self.fc = nn.Linear(out_features,16*16*base_channels*4)     #8 changed to 16 as we have 64x64 images instead of 32x32\n","    self.layer3 = BNReLUConv(base_channels*4,base_channels*2,pooling=True)\n","    self.layer2 = BNReLUConv(base_channels*2,base_channels,pooling=True)\n","    self.layer1 = BNReLUConv(base_channels,1,pooling=False)\n","  \n","  def forward(self,x):\n","    out = self.fc(x)\n","    out = out.view(x.shape[0],self.base_channels*4,16,16)       #8 changed to 16 as we have 64x64 images instead of 32x32\n","    out = self.layer3(out)\n","    out = self.layer2(out)\n","    out = self.layer1(out)\n","    return torch.sigmoid(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655741317176,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"s__yF3ze5lH8","outputId":"8d290474-c9b4-44cb-d19f-748b6f9b3d26"},"outputs":[{"name":"stdout","output_type":"stream","text":["CK VAE Definition\n","VAE(\n","  (encoder): Encoder(\n","    (layer1): ConvBNReLU(\n","      (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (layer2): ConvBNReLU(\n","      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (layer3): ConvBNReLU(\n","      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (fc): Linear(in_features=16384, out_features=128, bias=True)\n","  )\n","  (decoder): Decoder(\n","    (fc): Linear(in_features=64, out_features=16384, bias=True)\n","    (layer3): BNReLUConv(\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (pool): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n","    )\n","    (layer2): BNReLUConv(\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (pool): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n","    )\n","    (layer1): BNReLUConv(\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n",")\n"]}],"source":["class VAE(nn.Module):\n","  def __init__(self, out_features=32,base_channels=16):\n","    super(VAE, self).__init__()\n","    # Initialize the encoder and decoder using a dimensionality out_features for the vector z\n","    self.out_features = out_features\n","    self.encoder = Encoder(out_features*2,base_channels)\n","    self.decoder = Decoder(out_features,base_channels)\n","\n","  # function to obtain the mu and sigma of z for a samples x\n","  def encode(self,x):\n","    aux = self.encoder(x)\n","    # get z mean\n","    z_mean = aux[:,0:self.out_features]\n","    # get z variance\n","    z_log_var = aux[:,self.out_features::]\n","    return z_mean, z_log_var\n","\n","  # function to generate a random sample z given mu and sigma\n","  def sample_z(self,z_mean,z_log_var):\n","    z_std = z_log_var.mul(0.5).exp()\n","    samples_unit_normal = torch.randn_like(z_mean)\n","    samples_z = samples_unit_normal*z_std + z_mean\n","    return samples_z\n","\n","  # (1) encode a sample \n","  # (2) obtain a random vector z from mu and sigma\n","  # (3) Reconstruct the image using the decoder\n","  def forward(self,x):\n","    z_mean, z_log_var = self.encode(x)\n","    samples_z = self.sample_z(z_mean,z_log_var)\n","    x_rec = self.decoder(samples_z)\n","    return x_rec, z_mean, z_log_var\n","\n","# Print summary of the mode\n","print('CK VAE Definition')\n","vae = VAE(64)\n","print(vae)"]},{"cell_type":"markdown","metadata":{"id":"YCfPKp-h5_zS"},"source":["**Train function for VAE**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77yJ9p8X8dnz"},"outputs":[],"source":["def vae_vis_task(model_name, epoch):\n","  # Load trained VAE\n","  vae = VAE(64)\n","  vae.eval() # Evaluation mode for the model\n","  vae.load_state_dict(torch.load(results_path+'/'+model_name)) # Load model\n","  vae = vae.to(device)\n","\n","  ### Generate Images from random noise z ###\n","  n_samples = 16\n","\n","  # Random vectors z~N(0,I)\n","  z = torch.randn((n_samples,vae.out_features)).to(device)\n","\n","  # Genearte images with the decoder from the random vectors\n","  x_rec = vae.decoder(z)\n","\n","  # Show synthetic images\n","  plt.figure(figsize=(9,9))\n","  plt.title('Generated Images from random noise z in Epoch '+str(epoch+1))\n","  image_grid = make_grid(x_rec.cpu(),nrow=4,padding=1)\n","  plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","  plt.show()\n","\n","  ### Interpolating over generated images ###\n","  n_iterpolations =50\n","\n","  # Sample a set of pairs z_init and z_final\n","  z_init = torch.randn((n_samples,vae.out_features)).to(device)*2\n","  z_final = torch.randn((n_samples,vae.out_features)).to(device)*2\n","\n","  # Compute interpolations between z_init and z_final\n","  # and generate an image for each interpolation.\n","  interpolation_images = []\n","  for interp in range(0,n_iterpolations):\n","    interp_0_1 = float(interp) / (n_iterpolations-1)\n","    z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","    x_rec = vae.decoder(z)\n","    image_grid = make_grid(x_rec.cpu(),nrow=4,padding=1)\n","    image_grid = image_grid.permute(1,2,0).detach().numpy()\n","    # save the generated images in a list\n","    interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","\n","  # Concatenate the inversion of the list to generate a \"loop\" animation\n","  interpolation_images += interpolation_images[::-1]\n","\n","  # Generate and visualize a give showing the interpolation results.\n","  imname = results_path+'/vae_interpolation_ck_E'+str(epoch+1)+'.gif'\n","  imageio.mimsave(imname, interpolation_images, fps=25)\n","\n","  with open(imname,'rb') as f:\n","    display(ImageDisp(data=f.read(), format='png',width=512,height=512))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QXhYY-w5vbY"},"outputs":[],"source":["## Kullback-Leibler regularization computation\n","def kl_divergence(z_mean,z_log_var):\n","  kl_loss = 0.5 * torch.sum(  (torch.exp(z_log_var) + z_mean**2 - 1.0 - z_log_var),axis=1)\n","  return kl_loss.mean()\n","\n","# Train function\n","def train_VAE(vae,  train_loader, optimizer, kl_weight=0.001, num_epochs=10, model_name='vae_ck.ckpt', device='cpu'):\n","    vae.to(device)\n","    vae.train() # Set the model in train mode\n","    total_step = len(train_loader)\n","    losses_list = []\n","    criterion = nn.MSELoss() # Use mean-squared error to compare the original and reconstructe images\n","    \n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        rec_loss_avg = 0\n","        kl_loss_avg = 0\n","        nBatches = 0\n","        for i, images in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            images = images.to(device)\n","\n","            # Forward pass (get encoder variables and reconstructed images)\n","            x_rec, z_mean, z_log_var = vae(images)\n","\n","            reconstruction_loss = criterion(x_rec, images) # Reconstruction loss (x,x_rec)\n","            kl_loss = kl_divergence(z_mean, z_log_var) # Compute KL divergecnes KL( N(mu_x,sigma_x) || N(0,I))\n","            \n","            # Backward and optimize reconstruction loss and kl regularization\n","            optimizer.zero_grad()\n","            loss = reconstruction_loss + kl_loss*kl_weight # we use a weight to balance the importance of the KL loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            rec_loss_avg += reconstruction_loss.cpu().item()\n","            kl_loss_avg += kl_loss.cpu().item()\n","\n","            nBatches+=1\n","            if (i+1) % 100 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        losses_list.append(rec_loss_avg / nBatches)\n","        # save trained model\n","        torch.save(vae.state_dict(), results_path+ '/' + model_name)\n","\n","        #visualization task every two epochs\n","        if ((epoch+1)%2 == 0 and epoch != 0):\n","          vae_vis_task(model_name, epoch)\n","        \n","    return losses_list "]},{"cell_type":"markdown","metadata":{"id":"elWdH7HM6jzC"},"source":["**Train VAE**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"14klZ_5_nodYI0pz-YZqRfCS4D1PGqnZd"},"executionInfo":{"elapsed":439236,"status":"ok","timestamp":1655741756404,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"1F4FKSy96bIp","outputId":"121763c7-5ca2-4d85-959c-22b1a11c26e7"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["# Training a VAE on MNIST: z has 32 dimensions\n","# We use Adam optimizer which is tipically used in VAEs and GANs\n","\n","vae = VAE(64)\n","kl_weight=0.001 \n","\n","#Initialize optimizer \n","learning_rate = .001\n","optimizer = torch.optim.Adam(vae.parameters(),lr = learning_rate, weight_decay=1e-5)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","loss_list = train_VAE(vae, train_loader, optimizer, kl_weight=kl_weight,\n","                      num_epochs=20, model_name='vae_ck.ckpt', device=device)"]},{"cell_type":"markdown","metadata":{"id":"FhQbpBkQ5ux0"},"source":["# Ex. 2"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1655742747256,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"J5zT1eRf5ux4","outputId":"f2504046-223c-4d62-f565-9c2d6c3153d9"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\\n2. For every two epochs during training:\\n  2.1. Generate and show a set of images from random noise z. \\n  2.2. Visualize a set of generated images by interpolating over the latent space z.\\n  2.3 Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\\nCompare the results with the ones obtained with VAEs\\n'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Generate and show a set of images from random noise z. \n","  2.2. Visualize a set of generated images by interpolating over the latent space z.\n","  2.3 Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\n","Compare the results with the ones obtained with VAEs\n","'''"]},{"cell_type":"markdown","metadata":{"id":"2v8IoCDp6Aif"},"source":["## Sol. 2"]},{"cell_type":"markdown","metadata":{"id":"BsHMb5Hm7iIT"},"source":["GAN definition"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655742747631,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"XufCZ-VU7pOH"},"outputs":[],"source":["# Discriminator similar to VAE encoder\n","class Discriminator(nn.Module):\n","  def __init__(self, base_channels=16):\n","    super(Discriminator, self).__init__()\n","    # last fully connected layer acts as a a binary classifier\n","    self.classifier = Encoder(1,base_channels)\n","\n","  # Forward pass obtaining the discriminator probability\n","  def forward(self,x):\n","    out = self.classifier(x)\n","    # use sigmoid to get the real/fake image probability\n","    return torch.sigmoid(out)\n","\n","# Generator is defined as VAE decoder\n","class Generator(nn.Module):\n","  def __init__(self,in_features,base_channels=16):\n","    super(Generator, self).__init__()\n","    self.base_channels = base_channels\n","    self.in_features = in_features\n","    self.decoder = Decoder(in_features,base_channels)\n","\n","  # Generate an image from vector z\n","  def forward(self,z):\n","    return torch.sigmoid(self.decoder(z))\n","\n","  # Sample a set of images from random vectors z\n","  def sample(self,n_samples=256,device=device):\n","    samples_unit_normal = torch.randn((n_samples,self.in_features)).to(device)\n","    return self.decoder(samples_unit_normal)"]},{"cell_type":"markdown","metadata":{"id":"n1qCAAM37wQm"},"source":["GAN Train Function"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655742747632,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"JczAO6eV72XV"},"outputs":[],"source":["# GAN Train function. We have a generator and discriminator models and their respective optimizers.\n","def train_GAN(gen, disc,  train_loader, optimizer_gen, optim_disc,\n","              num_epochs=10, model_name='gan_faces.ckpt', device=device):\n","    gen = gen.to(device)\n","    gen.train() # Set the generator in train mode\n","    disc = disc.to(device)\n","    disc.train() # Set the discriminator in train mode\n","\n","    total_step = len(train_loader)\n","    losses_list = []\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        disc_loss_avg = 0\n","        gen_loss_avg = 0\n","        nBatches = 0\n","        update_generator = True\n","\n","        for i, (real_images) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            real_images = real_images.to(device)\n","            n_images = real_images.shape[0]\n","\n","            # Forward pass\n","            # Generate random images with the generator\n","            fake_images = gen.sample(n_images,device=device)\n","            \n","            # Use the discriminator to obtain the probabilties for real and generate imee\n","            prob_real = disc(real_images)\n","            prob_fake = disc(fake_images)\n","            \n","            # Generator loss\n","            gen_loss = -torch.log(prob_fake).mean()\n","            # Discriminator loss\n","            disc_loss = -0.5*(torch.log(prob_real) + torch.log(1-prob_fake)).mean()\n","\n","            \n","            # We are going to update the discriminator and generator parameters alternatively at each iteration\n","\n","            if(update_generator):\n","              # Optimize generator\n","              # Backward and optimize\n","              optimizer_gen.zero_grad()\n","              gen_loss.backward() # Necessary to not erase intermediate variables needed for computing disc_loss gradient\n","              optimizer_gen.step()\n","              update_generator = False\n","            else:           \n","              # Optimize discriminator\n","              # Backward and optimize\n","              optimizer_disc.zero_grad()\n","              disc_loss.backward()\n","              optimizer_disc.step()\n","              update_generator = True\n","                \n","\n","            disc_loss_avg += disc_loss.cpu().item()\n","            gen_loss_avg += gen_loss.cpu().item()\n","\n","            nBatches+=1\n","            if (i+1) % 200 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","        # Save model\n","        losses_list.append(disc_loss_avg / nBatches)\n","        torch.save(gan_gen.state_dict(), results_path+ '/' + model_name)\n","\n","        # visualisation task\n","        if ((epoch+1)%2 == 0 and epoch != 0):\n","          gan_vis_task(model_name, epoch)\n","          \n","    return losses_list "]},{"cell_type":"markdown","metadata":{"id":"4qA-f2z376Tk"},"source":["Training a GAN"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655742747632,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"aBR26Cis7c88"},"outputs":[],"source":["def gan_vis_task(model_name, epoch):\n","  # Load generator\n","  gan_gen = Generator(64)\n","  gan_gen.load_state_dict(torch.load(results_path+'/'+model_name))\n","  gan_gen.eval() # Put in eval model\n","  gan_gen = gan_gen.to(device)\n","\n","  # Generate random images from sampled vectors z and visualize them \n","  x_gen = gan_gen.sample(16,device=device)\n","  image_grid = make_grid(x_gen.cpu(),nrow=4,padding=1)\n","  plt.figure(figsize=(8,8))\n","  plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","  plt.show()\n","\n","  n_samples = 16\n","  n_iterpolations =50\n","\n","  z_init = torch.randn((n_samples,vae.out_features)).to(device)\n","  z_final = torch.randn((n_samples,vae.out_features)).to(device)\n","\n","  interpolation_images = []\n","  for interp in range(0,n_iterpolations):\n","    interp_0_1 = float(interp) / (n_iterpolations-1)\n","    z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","    x_rec = gan_gen.decoder(z.to(device))\n","    image_grid = make_grid(x_rec.cpu(),nrow=4,padding=1)\n","    image_grid = image_grid.permute(1,2,0).detach().numpy()\n","\n","    interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","  interpolation_images += interpolation_images[::-1]\n","\n","  imname = results_path+'/gan_interpolation_ck_E'+str(epoch+1)+'.gif'\n","  imageio.mimsave(imname, interpolation_images, fps=25)\n","\n","  with open(imname,'rb') as f:\n","    display(ImageDisp(data=f.read(), format='png',width=512,height=512))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1kL84VYa3wIOsy4NvJsZqqK76YyKjN1SF"},"executionInfo":{"elapsed":477364,"status":"ok","timestamp":1655743345547,"user":{"displayName":"Arne Berresheim","userId":"12671228248766843862"},"user_tz":-120},"id":"BQGooUNZ786c","outputId":"80e5fcf5-c610-4e4f-bff3-db0257194403"},"outputs":[],"source":["# Define Geneartor and Discriminator networks\n","gan_gen = Generator(64)\n","gan_disc = Discriminator()\n","\n","#Initialize indepdent optimizer for both networks\n","learning_rate = .0005\n","optimizer_gen = torch.optim.Adam(gan_gen.parameters(),lr = learning_rate, weight_decay=1e-5)\n","optimizer_disc = torch.optim.Adam(gan_disc.parameters(),lr = learning_rate, weight_decay=1e-5)\n","\n","# Train the GAN\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","loss_list = train_GAN(gan_gen,gan_disc, train_loader, optimizer_gen, optimizer_disc,\n","                      num_epochs=20, model_name='gan_faces.ckpt', device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["EvWs2XIYNfhg"],"machine_shape":"hm","name":"P4-Questions.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}